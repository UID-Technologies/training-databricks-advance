## ğŸ§ª Lab 1 â€“ Introduction to Data Engineering in Databricks

**Covers:** â€œData Engineering in Databricksâ€ (concept + hands-on)
**Estimated Time:** 45â€“60 minutes

### ğŸ¯ Learning Objectives

By the end of this lab, learners will:

* Understand what a **cluster** and **notebook** are.
* Run basic **PySpark** and **SQL** code.
* See how data engineering = **ingestion â†’ transform â†’ write** in Databricks.

### âœ… Prerequisites

* Databricks workspace access.
* Permission to create a **cluster** and **notebooks**.
* Any cloud (Azure/AWS/GCP) is fine.

---

### Step 1 â€“ Log In & Understand the Home Screen

1. Open the Databricks URL provided by the trainer.
2. After login, quickly identify:

   * **Workspace** / **Catalog** icon.
   * **Compute** (for clusters).
   * **Workflows / Jobs**.
3. Explain (trainer note):
   â€œNotebooks are like Jupyter. Clusters are where code runs. Data Engineering = building notebooks + pipes + jobs.â€

---

### Step 2 â€“ Create a Cluster

1. Go to **Compute** â†’ **Create Compute** (or **Create Cluster**).
2. Recommended for lab:

   * Name: `de-training-cluster`
   * Mode: **Single User** or **Standard**
   * Node type: any small default (e.g., `2X-Small`).
   * Runtime: choose a recent **DBR with SQL & Python**.
3. Click **Create**.
4. Wait until state = **Running**.

> Trainer tip: Ask them, *â€œWhat do you think happens if cluster is stopped when you run a notebook?â€* â€“ to reinforce the concept.

---

### Step 3 â€“ Create Your First Notebook

1. Go to **Workspace** â†’ select your user folder.
2. Click **Create** â†’ **Notebook**.
3. Set:

   * Name: `01_Intro_Data_Engineering`
   * Language: **Python**
   * Attach to: `de-training-cluster` (created above).
4. In the first cell, paste:

```python
spark
```

5. Run the cell (Shift + Enter) â€“ they should see a SparkSession printout.

---

### Step 4 â€“ Run Simple PySpark Code

1. In a new cell, run:

```python
df = spark.range(1, 101)   # numbers 1 to 100
df.show(10)
```

2. Ask them:

   * What is a DataFrame?
   * What does `show(10)` do?

3. In a new cell:

```python
df.count()
```

4. In a new cell:

```python
df_filtered = df.where("id % 2 == 0")  # even numbers
df_filtered.show(10)
```

---

### Step 5 â€“ Use SQL in the Same Notebook

1. Add a new **SQL** cell by starting with `%sql`:

```sql
%sql
SELECT COUNT(*) AS total_rows FROM range(1, 101);
```

2. Show them:

   * Same logical operation using SQL.
   * Highlight â€œSingle Engine, Multi-languageâ€.

---

### Step 6 â€“ Save Results as a Table (First Simple Pipeline)

1. Convert the range DataFrame into a table:

```python
df.write.mode("overwrite").saveAsTable("lab.bronze_numbers")
```

> If Unity Catalog is enabled, you may need:
> `catalog.schema.table` â†’ e.g. `training.bronze.numbers`.

2. Confirm via SQL:

```sql
%sql
SELECT * FROM lab.bronze_numbers LIMIT 10;
```

3. Explain:

   * This is a **mini pipeline**:

     * Generate data â†’ write to table â†’ query via SQL.

---

### Step 7 â€“ Wrap-Up Discussion

Ask participants:

* How is this different from writing scripts on plain Spark clusters?
* Where do notebooks, clusters, and tables fit into â€œData Engineering in Databricksâ€?

---
