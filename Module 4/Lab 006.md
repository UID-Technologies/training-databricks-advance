Below is a **trainer-grade, Databricks-aligned** explanation + **step-by-step lab exercises** for all three topics:

**Compound AI Systems**,
**Multi-Stage Reasoning Chains**,
**Agents & Cognitive Architectures**.

Everything is written for **Databricks Trial Edition**, so no assumptions about Mosaic Agent Framework private preview or Workspace assets.

---

# ‚úÖ **1. FOUNDATIONS OF COMPOUND AI SYSTEMS**

## **1.1 What Are Compound AI Systems? (Definition)**

A **Compound AI System** is an AI application built by *combining multiple AI components* such as:

* LLMs
* Vector Search
* Tools (SQL, APIs, functions)
* RAG pipelines
* Agents
* Knowledge bases
* Rule-based systems
* Orchestrators / DAGs (Workflows)

Think of it as:
**‚ÄúLLMs + Tools + Knowledge + Workflows = Advanced AI Application‚Äù**

### Examples:

| Compound AI System     | Components                                 |
| ---------------------- | ------------------------------------------ |
| RAG Chatbot            | LLM + Vector Search + Embeddings + Chunker |
| Automated Data Analyst | SQL Agent + LLM Planner + Tables           |
| Customer Support Bot   | LLM + RAG + Workflow + Tools               |
| Compliance Checker     | LLM + PDF extractor + Policy rules engine  |

Compound systems let you build **production-grade, predictable, trustworthy AI**, not just ‚Äúchatbots‚Äù.

---

## **1.2 Designing Compound AI Systems (Architecture)**

A typical architecture has 5 layers:

### **Layer 1 ‚Äî Data & Context Layer**

* Enterprise data
* Document stores
* Delta Lake / Vector Store

### **Layer 2 ‚Äî Embeddings & Search Layer**

* Chunking
* Embeddings
* Vector Search (Mosaic AI Vector Search)

### **Layer 3 ‚Äî Retrieval Layer**

* RAG
* Context building
* Document ranking

### **Layer 4 ‚Äî Reasoning Layer**

* LLM orchestration
* Chains (multi-step)
* Planning

### **Layer 5 ‚Äî Action/Tool Layer**

* SQL execution
* File operations
* Agent tools
* Function calling

### **Layer 6 ‚Äî Orchestration Layer**

* Databricks Workflows
* MLflow for components
* Serving endpoints

---

# üß™ **LAB 1 ‚Äî Build a Basic Compound AI System in Databricks (Trial Edition)**

### **Goal:**

Build a tiny compound system:
**PDF Extractor ‚Üí Chunker ‚Üí Vector Store ‚Üí Retriever ‚Üí LLM**

### **Notebook:** `compound_ai_system_basics`

---

### **STEP 1 ‚Äî Extract PDF**

```python
import fitz
pdf = fitz.open("/dbfs/FileStore/rag_project/rag_enterprise_knowledge_base.pdf")
text = "".join([page.get_text() for page in pdf])
print(text[:800])
```

---

### **STEP 2 ‚Äî Chunk + Embed + Store in Delta**

```python
from sentence_transformers import SentenceTransformer
import pandas as pd

chunks = text.split(". ")   # simple chunking
embedder = SentenceTransformer("all-MiniLM-L6-v2")

df = pd.DataFrame({
    "chunk": chunks,
    "embedding": embedder.encode(chunks).tolist()
})

spark.createDataFrame(df).write.mode("overwrite").format("delta") \
    .save("/FileStore/compound/chunks")
```

---

### **STEP 3 ‚Äî Retrieve Relevant Chunks**

```python
import numpy as np

query = "What is Unity Catalog?"
q_emb = embedder.encode([query])[0]

pdf = spark.read.format("delta").load("/FileStore/compound/chunks").toPandas()
pdf["score"] = pdf["embedding"].apply(lambda x: float(np.dot(x, q_emb)))
pdf.sort_values("score", ascending=False).head(3)
```

---

### **RESULT:**

You have now built a **Compound AI retrieval system**.

---

# ‚úÖ **2. BUILDING MULTI-STAGE REASONING CHAINS**

## **2.1 Introduction**

A **reasoning chain** is a sequence of deliberate steps an AI system performs to solve a problem:

### Example chain for ‚ÄúSummarize PDF ‚Üí Extract KPIs ‚Üí Create SQL Query‚Äù

1. Ingest and extract PDF text
2. Chunk the text
3. Build embeddings
4. Retrieve relevant text
5. Ask LLM to extract KPIs
6. Ask LLM to generate SQL
7. Ask SQL engine to run
8. Ask LLM to summarize result

This is much more powerful than a single-shot LLM prompt.

---

## **2.2 Databricks Products That Assist Multi-Stage Reasoning Chaining**

| Databricks Feature            | How It Helps                                                  |
| ----------------------------- | ------------------------------------------------------------- |
| **MLflow**                    | Track each stage (embedding model, chunking rules, RAG logic) |
| **Workflows**                 | Orchestrate multi-step reasoning DAGs                         |
| **Delta Live Tables**         | Automated ingestion for reasoning contexts                    |
| **Mosaic AI Vector Search**   | Retrieval stage                                               |
| **Model Serving**             | LLM inference as a callable step                              |
| **DBRX / Llama / API Access** | Final reasoning layer                                         |

**You build chains ‚Üí Databricks executes them reliably.**

---

# üß™ **LAB 2 ‚Äî Multi-Stage Reasoning Chain in Databricks**

### **Goal:**

Build a 4-step chain:

1. Extract PDF text
2. Retrieve relevant chunks
3. Ask LLM to summarize
4. Ask LLM to convert summary into bullet list

### **Notebook:** `multi_stage_reasoning_chain`

---

### **STEP 1 ‚Äî Extract + Retrieve (Reuse from Lab 1)**

Use same PDF ‚Üí chunk ‚Üí embed pipeline.

---

### **STEP 2 ‚Äî Stage 1 ‚Äî Summarization**

```python
summary_prompt = f"""
Summarize the following enterprise knowledge:

{pdf_text}

Summary:
"""

summary = summary_prompt   # call DBRX/OpenAI if available
print(summary)
```

---

### **STEP 3 ‚Äî Stage 2 ‚Äî Extract Key Insights**

```python
kpi_prompt = f"""
Convert this summary into 5 bullet points:

{summary}

Output:
"""

bulletized = kpi_prompt
print(bulletized)
```

This is a **2-step reasoning chain**.

---

### **STEP 4 ‚Äî Automate Multi-Stage Chain Using Functions**

```python
def summarize(text):
    return f"Summary of text: {text[:400]}..."

def extract_bullets(summary):
    return f"- Bullet 1 from {summary[:60]}..."

summary = summarize(pdf_text)
bullets = extract_bullets(summary)

print(summary)
print(bullets)
```

In production you‚Äôd replace functions with **actual model inference**.

---

# ‚úÖ **3. AGENTS & COGNITIVE ARCHITECTURE**

## **3.1 Introduction**

An **AI Agent** is an LLM-powered program that:

* Thinks
* Plans
* Executes tools
* Observes
* Loops until goal is achieved

Think of it as:
**LLM + Tools + Memory + Policy + Environment**

That is a *cognitive system*, not just a chatbot.

---

## **3.2 Tools for Building Agents (Databricks Edition)**

While Databricks doesn‚Äôt yet have a ‚Äútool calling framework‚Äù in Trial, you can build agent-style systems using:

| Component                | Purpose                                          |
| ------------------------ | ------------------------------------------------ |
| **Python Functions**     | Tools (SQL executor, file search, vector search) |
| **MLflow PyFunc Models** | Agent memory + logic                             |
| **Vector Search**        | Agent Retrieval Tool                             |
| **Model Serving**        | Agent brain (LLM)                                |
| **Workflows**            | Agent orchestration loop                         |
| **Volumes / Delta**      | Agent memory log                                 |

---

## **3.3 Multi-Modal AI (Images, PDFs, Text)**

A modern agent can use:

* Text models
* Embedding models
* PDF parsers
* Image models (OCR / Vision)
* Speech models (ASR / TTS)

**Databricks Trial** supports:

‚úî PDF ingestion
‚úî Image ingestion (manual)
‚úî Vision models via HuggingFace
‚úî LLM context mixing (multi-modal prompt)

---

# üß™ **LAB 3 ‚Äî Build a Simple Agent With Tools (Databricks Trial)**

### **Goal:**

Create a mini-agent that:

1. Receives a user question
2. Decides which tool to use:

   * PDF Search
   * Vector Search
   * FAQ Lookup
3. Returns a final answer

### **Notebook:** `simple_agent_system`

---

## **STEP 1 ‚Äî Define Tools**

### **Tool A ‚Äî FAQ Lookup**

```python
import pandas as pd

faq = pd.read_csv("/dbfs/FileStore/rag_project/raw/faq.csv")

def faq_lookup(query):
    for _, row in faq.iterrows():
        if query.lower() in row["question"].lower():
            return row["answer"]
    return None
```

---

### **Tool B ‚Äî Vector Search**

Reuse the FAISS index from Lab 3:

```python
def vector_tool(query):
    results = semantic_search(query, k=3)
    return "\n".join([r["chunk"] for r in results])
```

---

### **Tool C ‚Äî Full PDF Search**

```python
def pdf_search(query):
    if query.lower() in text.lower():
        return "Found in PDF: " + query
    return None
```

---

## **STEP 2 ‚Äî Agent Reasoning: Choose Tool**

```python
def agent_decide(question):
    if "what" in question.lower() or "how" in question.lower():
        return "vector"
    if "faq" in question.lower():
        return "faq"
    return "pdf"
```

---

## **STEP 3 ‚Äî Agent Execution**

```python
def agent_run(question):
    tool = agent_decide(question)
    
    if tool == "faq":
        answer = faq_lookup(question)
    elif tool == "vector":
        answer = vector_tool(question)
    else:
        answer = pdf_search(question)
    
    if not answer:
        answer = "I don't know."
    
    return {
        "question": question,
        "chosen_tool": tool,
        "answer": answer
    }

agent_run("What is Delta Lake?")
```

---

## **STEP 4 ‚Äî Add LLM for Final Answer Synthesis**

```python
context = agent_run("Explain Unity Catalog")["answer"]

llm_prompt = f"""
Use the following retrieved context to answer:

{context}

Final Answer:
"""

llm_prompt   # Call LLM if available
```

This creates a **multi-tool agent**.

---

# üéâ COMPLETED:

You now have:

‚úî Foundations of Compound AI Systems
‚úî Multi-Stage Reasoning Chains (with Databricks support)
‚úî Agents + Cognitive Architecture (with tools & reasoning)
‚úî 3 hands-on labs aligned with Databricks Trial Edition

---

